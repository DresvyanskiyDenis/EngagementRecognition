{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "from scipy.stats import stats\n",
    "\n",
    "from src.NoXi.preprocessing.labels_preprocessing import load_all_labels_by_paths"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# load labels\n",
    "path_to_labels=r'E:\\Databases\\NoXi\\NoXi_annotations_reliable_gold_standard_regression_with_additional_train_data'\n",
    "output_path=r'E:\\Databases\\NoXi\\NoXi_annotations_reliable_gold_standard_classification_with_additional_train_data'\n",
    "\n",
    "all_label_paths=glob.glob(os.path.join(path_to_labels,'**','**','**','annotation*.txt'))\n",
    "all_train_label_paths=glob.glob(os.path.join(path_to_labels,'**','train','**','annotation*.txt'))\n",
    "all_dev_label_paths=glob.glob(os.path.join(path_to_labels,'**','dev','**','annotation*.txt'))\n",
    "all_test_label_paths=glob.glob(os.path.join(path_to_labels,'**','test','**','annotation*.txt'))\n",
    "\n",
    "\n",
    "all_labels=load_all_labels_by_paths(all_label_paths)\n",
    "all_train_labels=load_all_labels_by_paths(all_train_label_paths)\n",
    "all_dev_labels=load_all_labels_by_paths(all_dev_label_paths)\n",
    "all_test_labels=load_all_labels_by_paths(all_test_label_paths)\n"
   ],
   "execution_count": 62,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [
    {
     "data": {
      "text/plain": "{'E:\\\\Databases\\\\NoXi\\\\NoXi_annotations_reliable_gold_standard_regression_with_additional_train_data\\\\English\\\\train\\\\026_2016-04-06_Nottingham\\\\annotation_expert.txt': array([0.402416, 0.405325, 0.405499, ..., 0.630929, 0.636819, 0.636116],\n       dtype=float32),\n 'E:\\\\Databases\\\\NoXi\\\\NoXi_annotations_reliable_gold_standard_regression_with_additional_train_data\\\\English\\\\train\\\\026_2016-04-06_Nottingham\\\\annotation_novice.txt': array([0.250783, 0.289286, 0.290491, ..., 0.429497, 0.427062, 0.427217],\n       dtype=float32),\n 'E:\\\\Databases\\\\NoXi\\\\NoXi_annotations_reliable_gold_standard_regression_with_additional_train_data\\\\English\\\\train\\\\027_2016-04-06_Nottingham\\\\annotation_expert.txt': array([0.5, 0.5, 0.5, ..., 0.5, 0.5, 0.5], dtype=float32),\n 'E:\\\\Databases\\\\NoXi\\\\NoXi_annotations_reliable_gold_standard_regression_with_additional_train_data\\\\English\\\\train\\\\027_2016-04-06_Nottingham\\\\annotation_novice.txt': array([0.5   , 0.5   , 0.5   , ..., 0.6875, 0.6875, 0.625 ], dtype=float32),\n 'E:\\\\Databases\\\\NoXi\\\\NoXi_annotations_reliable_gold_standard_regression_with_additional_train_data\\\\English\\\\train\\\\029_2016-04-06_Nottingham\\\\annotation_expert.txt': array([0.25 , 0.25 , 0.25 , ..., 0.375, 0.375, 0.375], dtype=float32),\n 'E:\\\\Databases\\\\NoXi\\\\NoXi_annotations_reliable_gold_standard_regression_with_additional_train_data\\\\English\\\\train\\\\029_2016-04-06_Nottingham\\\\annotation_novice.txt': array([0.333333, 0.333333, 0.333333, ..., 0.333333, 0.333333, 0.333333],\n       dtype=float32),\n 'E:\\\\Databases\\\\NoXi\\\\NoXi_annotations_reliable_gold_standard_regression_with_additional_train_data\\\\English\\\\train\\\\030_2016-04-06_Nottingham\\\\annotation_expert.txt': array([0.5 , 0.5 , 0.5 , ..., 0.75, 0.75, 0.75], dtype=float32),\n 'E:\\\\Databases\\\\NoXi\\\\NoXi_annotations_reliable_gold_standard_regression_with_additional_train_data\\\\English\\\\train\\\\030_2016-04-06_Nottingham\\\\annotation_novice.txt': array([0.5, 0.5, 0.5, ..., 0.5, 0.5, 0.5], dtype=float32),\n 'E:\\\\Databases\\\\NoXi\\\\NoXi_annotations_reliable_gold_standard_regression_with_additional_train_data\\\\English\\\\train\\\\039_2016-04-07_Nottingham\\\\annotation_expert.txt': array([0.5   , 0.5   , 0.5   , ..., 0.1875, 0.125 , 0.    ], dtype=float32),\n 'E:\\\\Databases\\\\NoXi\\\\NoXi_annotations_reliable_gold_standard_regression_with_additional_train_data\\\\English\\\\train\\\\052_2016-04-12_Nottingham\\\\annotation_expert.txt': array([0.5     , 0.5     , 0.501766, ..., 0.416667, 0.416667, 0.416667],\n       dtype=float32),\n 'E:\\\\Databases\\\\NoXi\\\\NoXi_annotations_reliable_gold_standard_regression_with_additional_train_data\\\\English\\\\train\\\\052_2016-04-12_Nottingham\\\\annotation_novice.txt': array([0.5     , 0.5     , 0.475305, ..., 0.5     , 0.5     , 0.5     ],\n       dtype=float32),\n 'E:\\\\Databases\\\\NoXi\\\\NoXi_annotations_reliable_gold_standard_regression_with_additional_train_data\\\\French\\\\train\\\\001_2016-03-17_Paris\\\\annotation_expert.txt': array([0.5     , 0.5     , 0.501486, ..., 0.5     , 0.5     , 0.5     ],\n       dtype=float32),\n 'E:\\\\Databases\\\\NoXi\\\\NoXi_annotations_reliable_gold_standard_regression_with_additional_train_data\\\\French\\\\train\\\\001_2016-03-17_Paris\\\\annotation_novice.txt': array([0.4375  , 0.4375  , 0.441763, ..., 0.5     , 0.5     , 0.5     ],\n       dtype=float32),\n 'E:\\\\Databases\\\\NoXi\\\\NoXi_annotations_reliable_gold_standard_regression_with_additional_train_data\\\\French\\\\train\\\\004_2016-03-18_Paris\\\\annotation_expert.txt': array([0.5     , 0.5     , 0.276215, ..., 0.5     , 0.5     , 0.5     ],\n       dtype=float32),\n 'E:\\\\Databases\\\\NoXi\\\\NoXi_annotations_reliable_gold_standard_regression_with_additional_train_data\\\\French\\\\train\\\\004_2016-03-18_Paris\\\\annotation_novice.txt': array([0.5    , 0.5    , 0.47791, ..., 0.5    , 0.5    , 0.5    ],\n       dtype=float32),\n 'E:\\\\Databases\\\\NoXi\\\\NoXi_annotations_reliable_gold_standard_regression_with_additional_train_data\\\\French\\\\train\\\\005_2016-03-18_Paris\\\\annotation_expert.txt': array([0.5, 0.5, 0.5, ..., 0.5, 0.5, 0.5], dtype=float32),\n 'E:\\\\Databases\\\\NoXi\\\\NoXi_annotations_reliable_gold_standard_regression_with_additional_train_data\\\\French\\\\train\\\\005_2016-03-18_Paris\\\\annotation_novice.txt': array([0.5, 0.5, 0.5, ..., 0.5, 0.5, 0.5], dtype=float32),\n 'E:\\\\Databases\\\\NoXi\\\\NoXi_annotations_reliable_gold_standard_regression_with_additional_train_data\\\\French\\\\train\\\\006_2016-03-18_Paris\\\\annotation_expert.txt': array([0.5, 0.5, 0.5, ..., 0.5, 0.5, 0.5], dtype=float32),\n 'E:\\\\Databases\\\\NoXi\\\\NoXi_annotations_reliable_gold_standard_regression_with_additional_train_data\\\\French\\\\train\\\\006_2016-03-18_Paris\\\\annotation_novice.txt': array([0.5, 0.5, 0.5, ..., 0.5, 0.5, 0.5], dtype=float32),\n 'E:\\\\Databases\\\\NoXi\\\\NoXi_annotations_reliable_gold_standard_regression_with_additional_train_data\\\\French\\\\train\\\\009_2016-03-25_Paris\\\\annotation_expert.txt': array([0.5, 0.5, 0.5, ..., 0.5, 0.5, 0.5], dtype=float32),\n 'E:\\\\Databases\\\\NoXi\\\\NoXi_annotations_reliable_gold_standard_regression_with_additional_train_data\\\\French\\\\train\\\\009_2016-03-25_Paris\\\\annotation_novice.txt': array([0.5, 0.5, 0.5, ..., 0.5, 0.5, 0.5], dtype=float32),\n 'E:\\\\Databases\\\\NoXi\\\\NoXi_annotations_reliable_gold_standard_regression_with_additional_train_data\\\\French\\\\train\\\\010_2016-03-25_Paris\\\\annotation_expert.txt': array([0.5  , 0.5  , 0.5  , ..., 0.375, 0.375, 0.375], dtype=float32),\n 'E:\\\\Databases\\\\NoXi\\\\NoXi_annotations_reliable_gold_standard_regression_with_additional_train_data\\\\French\\\\train\\\\010_2016-03-25_Paris\\\\annotation_novice.txt': array([0.5  , 0.5  , 0.5  , ..., 0.375, 0.375, 0.5  ], dtype=float32),\n 'E:\\\\Databases\\\\NoXi\\\\NoXi_annotations_reliable_gold_standard_regression_with_additional_train_data\\\\French\\\\train\\\\011_2016-03-25_Paris\\\\annotation_expert.txt': array([0.5     , 0.5     , 0.5     , ..., 0.333333, 0.333333, 0.333333],\n       dtype=float32),\n 'E:\\\\Databases\\\\NoXi\\\\NoXi_annotations_reliable_gold_standard_regression_with_additional_train_data\\\\French\\\\train\\\\011_2016-03-25_Paris\\\\annotation_novice.txt': array([0.5     , 0.5     , 0.333333, ..., 0.333333, 0.333333, 0.      ],\n       dtype=float32),\n 'E:\\\\Databases\\\\NoXi\\\\NoXi_annotations_reliable_gold_standard_regression_with_additional_train_data\\\\French\\\\train\\\\012_2016-03-25_Paris\\\\annotation_expert.txt': array([0.5 , 0.5 , 0.5 , ..., 0.25, 0.25, 0.25], dtype=float32),\n 'E:\\\\Databases\\\\NoXi\\\\NoXi_annotations_reliable_gold_standard_regression_with_additional_train_data\\\\French\\\\train\\\\012_2016-03-25_Paris\\\\annotation_novice.txt': array([0.5 , 0.5 , 0.5 , ..., 0.25, 0.25, 0.25], dtype=float32),\n 'E:\\\\Databases\\\\NoXi\\\\NoXi_annotations_reliable_gold_standard_regression_with_additional_train_data\\\\French\\\\train\\\\013_2016-03-30_Paris\\\\annotation_expert.txt': array([0.5 , 0.5 , 0.5 , ..., 0.25, 0.25, 0.25], dtype=float32),\n 'E:\\\\Databases\\\\NoXi\\\\NoXi_annotations_reliable_gold_standard_regression_with_additional_train_data\\\\French\\\\train\\\\013_2016-03-30_Paris\\\\annotation_novice.txt': array([0.5, 0.5, 0.5, ..., 0. , 0. , 0. ], dtype=float32),\n 'E:\\\\Databases\\\\NoXi\\\\NoXi_annotations_reliable_gold_standard_regression_with_additional_train_data\\\\French\\\\train\\\\014_2016-04-01_Paris\\\\annotation_expert.txt': array([0.5  , 0.5  , 0.5  , ..., 0.625, 0.625, 0.625], dtype=float32),\n 'E:\\\\Databases\\\\NoXi\\\\NoXi_annotations_reliable_gold_standard_regression_with_additional_train_data\\\\French\\\\train\\\\014_2016-04-01_Paris\\\\annotation_novice.txt': array([0.5  , 0.5  , 0.5  , ..., 0.625, 0.625, 0.625], dtype=float32),\n 'E:\\\\Databases\\\\NoXi\\\\NoXi_annotations_reliable_gold_standard_regression_with_additional_train_data\\\\German\\\\train\\\\067_2016-05-23_Augsburg\\\\annotation_novice.txt': array([0.5  , 0.5  , 0.5  , ..., 0.375, 0.375, 0.375], dtype=float32),\n 'E:\\\\Databases\\\\NoXi\\\\NoXi_annotations_reliable_gold_standard_regression_with_additional_train_data\\\\German\\\\train\\\\070_2016-05-23_Augsburg\\\\annotation_expert.txt': array([0.5, 0.5, 0.5, ..., 0. , 0. , 0. ], dtype=float32),\n 'E:\\\\Databases\\\\NoXi\\\\NoXi_annotations_reliable_gold_standard_regression_with_additional_train_data\\\\German\\\\train\\\\070_2016-05-23_Augsburg\\\\annotation_novice.txt': array([0.5 , 0.5 , 0.5 , ..., 0.  , 0.  , 0.25], dtype=float32),\n 'E:\\\\Databases\\\\NoXi\\\\NoXi_annotations_reliable_gold_standard_regression_with_additional_train_data\\\\German\\\\train\\\\073_2016-05-23_Augsburg\\\\annotation_expert.txt': array([0.5  , 0.5  , 0.5  , ..., 0.625, 0.625, 0.625], dtype=float32),\n 'E:\\\\Databases\\\\NoXi\\\\NoXi_annotations_reliable_gold_standard_regression_with_additional_train_data\\\\German\\\\train\\\\076_2016-05-24_Augsburg\\\\annotation_expert.txt': array([0.5  , 0.5  , 0.5  , ..., 0.375, 0.375, 0.375], dtype=float32),\n 'E:\\\\Databases\\\\NoXi\\\\NoXi_annotations_reliable_gold_standard_regression_with_additional_train_data\\\\German\\\\train\\\\076_2016-05-24_Augsburg\\\\annotation_novice.txt': array([0.5 , 0.5 , 0.5 , ..., 0.25, 0.25, 0.25], dtype=float32),\n 'E:\\\\Databases\\\\NoXi\\\\NoXi_annotations_reliable_gold_standard_regression_with_additional_train_data\\\\German\\\\train\\\\080_2016-05-24_Augsburg\\\\annotation_expert.txt': array([0.5     , 0.5     , 0.5     , ..., 0.416667, 0.416667, 0.416667],\n       dtype=float32),\n 'E:\\\\Databases\\\\NoXi\\\\NoXi_annotations_reliable_gold_standard_regression_with_additional_train_data\\\\German\\\\train\\\\080_2016-05-24_Augsburg\\\\annotation_novice.txt': array([0.5     , 0.5     , 0.5     , ..., 0.416667, 0.416667, 0.416667],\n       dtype=float32),\n 'E:\\\\Databases\\\\NoXi\\\\NoXi_annotations_reliable_gold_standard_regression_with_additional_train_data\\\\German\\\\train\\\\081_2016-05-24_Augsburg\\\\annotation_expert.txt': array([0.5     , 0.5     , 0.5     , ..., 0.666667, 0.666667, 0.666667],\n       dtype=float32),\n 'E:\\\\Databases\\\\NoXi\\\\NoXi_annotations_reliable_gold_standard_regression_with_additional_train_data\\\\German\\\\train\\\\081_2016-05-24_Augsburg\\\\annotation_novice.txt': array([0.5     , 0.5     , 0.5     , ..., 0.416667, 0.416667, 0.416667],\n       dtype=float32),\n 'E:\\\\Databases\\\\NoXi\\\\NoXi_annotations_reliable_gold_standard_regression_with_additional_train_data\\\\German\\\\train\\\\082_2016-05-25_Augsburg\\\\annotation_expert.txt': array([0.5, 0.5, 0.5, ..., 0. , 0. , 0. ], dtype=float32),\n 'E:\\\\Databases\\\\NoXi\\\\NoXi_annotations_reliable_gold_standard_regression_with_additional_train_data\\\\German\\\\train\\\\082_2016-05-25_Augsburg\\\\annotation_novice.txt': array([0.5, 0.5, 0.5, ..., 0. , 0. , 0. ], dtype=float32),\n 'E:\\\\Databases\\\\NoXi\\\\NoXi_annotations_reliable_gold_standard_regression_with_additional_train_data\\\\German\\\\train\\\\083_2016-05-25_Augsburg\\\\annotation_expert.txt': array([0.5, 0.5, 0.5, ..., 0.5, 0.5, 0.5], dtype=float32),\n 'E:\\\\Databases\\\\NoXi\\\\NoXi_annotations_reliable_gold_standard_regression_with_additional_train_data\\\\German\\\\train\\\\083_2016-05-25_Augsburg\\\\annotation_novice.txt': array([0.5 , 0.5 , 0.5 , ..., 0.75, 0.75, 0.75], dtype=float32)}"
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_train_labels"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h2> Labels softening (train) </h2>"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [],
   "source": [
    "\n",
    "def convert_to_soft_one_hot_encoding(value:float, class_values:Tuple[float,...])->np.ndarray:\n",
    "    \"\"\"Converts given value to the soft one-hot encoding vector, taking into account provided class_values.\n",
    "\n",
    "    We soft labels using the following procedure:\n",
    "    We have 5 classes - 0, 0.25, 0.5, 0.75, 1\n",
    "    If the label is between two classes, we create a weighted soft one-hot encoding according to the closeness to classes.\n",
    "    For example, for the value 0.7, we have two concurent classes - 0.25 and 0.5. TO calculate weights, we need to do the following:\n",
    "    Calculate the distance between classes 0.75 - 0.5 = 0.25. Calculate the \"location\" of the point on this segment via substracting the value of the lowest class from him 0.7 - 0.5 = 0.2.\n",
    "    To get the value of soft one-hot encoding for \"right\" class (counting from the point), we need to divide the \"location\" of the point on the distance 0.2/0.25=0.8. The value of \"left\" class is simply 1.-0.8 (calculated \"right\" value).\n",
    "    Thus, we got an one-hot encoding vector [0. 0. 0.2 0.8 0.] for the point 0.7\n",
    "\n",
    "    :param value: float\n",
    "            value of the point to convert to the soft one-hot encoding\n",
    "    :param class_values: Tuple[float,...]\n",
    "            the values, which classes can be equal to\n",
    "    :return: List[float]\n",
    "            soft one-hot encoding\n",
    "    \"\"\"\n",
    "    # check the requirements for the value variable\n",
    "    if value<0 or value>1:\n",
    "        raise Exception(\"The value of the variable \\\"value\\\" is more than 1 or less than 0\")\n",
    "    # find the indexes of the classes, between which the value lies\n",
    "    idx_right_class=next(i for i,v in enumerate(class_values) if value<=v)\n",
    "    idx_left_class=idx_right_class-1\n",
    "    # calculate the \"distance\" as it is described in the function description\n",
    "    distance=class_values[idx_right_class]-class_values[idx_left_class]\n",
    "    # calculate \"location\" as it is described in the function description\n",
    "    location=value-class_values[idx_left_class]\n",
    "    # create one-hot encoding vector with zeros\n",
    "    one_hot_vector=np.zeros(len(class_values))\n",
    "    # calculate the probability values of the one-hot encoding vector (for nearest right and left from the point classes)\n",
    "    one_hot_vector[idx_right_class]=location/distance\n",
    "    one_hot_vector[idx_left_class]=1.-one_hot_vector[idx_right_class]\n",
    "    return one_hot_vector\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [],
   "source": [
    "# TODO: Function for converting all labels to the soft ones (except for the test and validation set) done\n",
    "# TODO: Function for converting all test and validation labels to the categorical ones (in one-hot encoding appearance) - try for edge poitns (for example, 0.375) the Hamming window, or just a mode for the fixed-sized window.\n",
    "# TODO: save function"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [],
   "source": [
    "# converting all labels in training set to the soft one-hot encodings\n",
    "def convert_all_labels_to_soft_one_hot_encoding_by_paths(labels:Dict[str, np.ndarray], class_values:Tuple[float,...])->Dict[str, np.ndarray]:\n",
    "    \"\"\"Converts all labels presented in the Dict[path->values] to the soft one-hot encodings using provided class_values.\n",
    "\n",
    "    :param labels: Dict[str, np.ndarray]\n",
    "            labels in the format Dict[path->np.ndarray]\n",
    "    :param class_values: Tuple[float,...]\n",
    "            Possible values of the classes, ranked in ascended order\n",
    "    :return: Dict[str, np.ndarray]\n",
    "            converted to the soft one-hot encodings labels\n",
    "    \"\"\"\n",
    "    for key, values in labels.items():\n",
    "        labels[key]=np.array([convert_to_soft_one_hot_encoding(x, class_values) for x in values])\n",
    "    return labels"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [],
   "source": [
    "class_values=(0, 0.25, 0.5, 0.75, 1.)\n",
    "all_train_labels_converted=convert_all_labels_to_soft_one_hot_encoding_by_paths(all_train_labels, class_values)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [
    {
     "data": {
      "text/plain": "{'E:\\\\Databases\\\\NoXi\\\\NoXi_annotations_reliable_gold_standard_regression_with_additional_train_data\\\\English\\\\train\\\\026_2016-04-06_Nottingham\\\\annotation_expert.txt': array([[0.        , 0.39033604, 0.60966396, 0.        , 0.        ],\n        [0.        , 0.37870002, 0.62129998, 0.        , 0.        ],\n        [0.        , 0.37800395, 0.62199605, 0.        , 0.        ],\n        ...,\n        [0.        , 0.        , 0.47628403, 0.52371597, 0.        ],\n        [0.        , 0.        , 0.45272398, 0.54727602, 0.        ],\n        [0.        , 0.        , 0.45553589, 0.54446411, 0.        ]]),\n 'E:\\\\Databases\\\\NoXi\\\\NoXi_annotations_reliable_gold_standard_regression_with_additional_train_data\\\\English\\\\train\\\\026_2016-04-06_Nottingham\\\\annotation_novice.txt': array([[0.        , 0.99686801, 0.00313199, 0.        , 0.        ],\n        [0.        , 0.84285605, 0.15714395, 0.        , 0.        ],\n        [0.        , 0.83803594, 0.16196406, 0.        , 0.        ],\n        ...,\n        [0.        , 0.28201199, 0.71798801, 0.        , 0.        ],\n        [0.        , 0.29175198, 0.70824802, 0.        , 0.        ],\n        [0.        , 0.29113197, 0.70886803, 0.        , 0.        ]]),\n 'E:\\\\Databases\\\\NoXi\\\\NoXi_annotations_reliable_gold_standard_regression_with_additional_train_data\\\\English\\\\train\\\\027_2016-04-06_Nottingham\\\\annotation_expert.txt': array([[0., 0., 1., 0., 0.],\n        [0., 0., 1., 0., 0.],\n        [0., 0., 1., 0., 0.],\n        ...,\n        [0., 0., 1., 0., 0.],\n        [0., 0., 1., 0., 0.],\n        [0., 0., 1., 0., 0.]]),\n 'E:\\\\Databases\\\\NoXi\\\\NoXi_annotations_reliable_gold_standard_regression_with_additional_train_data\\\\English\\\\train\\\\027_2016-04-06_Nottingham\\\\annotation_novice.txt': array([[0.  , 0.  , 1.  , 0.  , 0.  ],\n        [0.  , 0.  , 1.  , 0.  , 0.  ],\n        [0.  , 0.  , 1.  , 0.  , 0.  ],\n        ...,\n        [0.  , 0.  , 0.25, 0.75, 0.  ],\n        [0.  , 0.  , 0.25, 0.75, 0.  ],\n        [0.  , 0.  , 0.5 , 0.5 , 0.  ]]),\n 'E:\\\\Databases\\\\NoXi\\\\NoXi_annotations_reliable_gold_standard_regression_with_additional_train_data\\\\English\\\\train\\\\029_2016-04-06_Nottingham\\\\annotation_expert.txt': array([[0. , 1. , 0. , 0. , 0. ],\n        [0. , 1. , 0. , 0. , 0. ],\n        [0. , 1. , 0. , 0. , 0. ],\n        ...,\n        [0. , 0.5, 0.5, 0. , 0. ],\n        [0. , 0.5, 0.5, 0. , 0. ],\n        [0. , 0.5, 0.5, 0. , 0. ]]),\n 'E:\\\\Databases\\\\NoXi\\\\NoXi_annotations_reliable_gold_standard_regression_with_additional_train_data\\\\English\\\\train\\\\029_2016-04-06_Nottingham\\\\annotation_novice.txt': array([[0.        , 0.66666806, 0.33333194, 0.        , 0.        ],\n        [0.        , 0.66666806, 0.33333194, 0.        , 0.        ],\n        [0.        , 0.66666806, 0.33333194, 0.        , 0.        ],\n        ...,\n        [0.        , 0.66666806, 0.33333194, 0.        , 0.        ],\n        [0.        , 0.66666806, 0.33333194, 0.        , 0.        ],\n        [0.        , 0.66666806, 0.33333194, 0.        , 0.        ]]),\n 'E:\\\\Databases\\\\NoXi\\\\NoXi_annotations_reliable_gold_standard_regression_with_additional_train_data\\\\English\\\\train\\\\030_2016-04-06_Nottingham\\\\annotation_expert.txt': array([[0., 0., 1., 0., 0.],\n        [0., 0., 1., 0., 0.],\n        [0., 0., 1., 0., 0.],\n        ...,\n        [0., 0., 0., 1., 0.],\n        [0., 0., 0., 1., 0.],\n        [0., 0., 0., 1., 0.]]),\n 'E:\\\\Databases\\\\NoXi\\\\NoXi_annotations_reliable_gold_standard_regression_with_additional_train_data\\\\English\\\\train\\\\030_2016-04-06_Nottingham\\\\annotation_novice.txt': array([[0., 0., 1., 0., 0.],\n        [0., 0., 1., 0., 0.],\n        [0., 0., 1., 0., 0.],\n        ...,\n        [0., 0., 1., 0., 0.],\n        [0., 0., 1., 0., 0.],\n        [0., 0., 1., 0., 0.]]),\n 'E:\\\\Databases\\\\NoXi\\\\NoXi_annotations_reliable_gold_standard_regression_with_additional_train_data\\\\English\\\\train\\\\039_2016-04-07_Nottingham\\\\annotation_expert.txt': array([[0.  , 0.  , 1.  , 0.  , 0.  ],\n        [0.  , 0.  , 1.  , 0.  , 0.  ],\n        [0.  , 0.  , 1.  , 0.  , 0.  ],\n        ...,\n        [0.25, 0.75, 0.  , 0.  , 0.  ],\n        [0.5 , 0.5 , 0.  , 0.  , 0.  ],\n        [1.  , 0.  , 0.  , 0.  , 0.  ]]),\n 'E:\\\\Databases\\\\NoXi\\\\NoXi_annotations_reliable_gold_standard_regression_with_additional_train_data\\\\English\\\\train\\\\052_2016-04-12_Nottingham\\\\annotation_expert.txt': array([[0.        , 0.        , 1.        , 0.        , 0.        ],\n        [0.        , 0.        , 1.        , 0.        , 0.        ],\n        [0.        , 0.        , 0.9929359 , 0.0070641 , 0.        ],\n        ...,\n        [0.        , 0.33333194, 0.66666806, 0.        , 0.        ],\n        [0.        , 0.33333194, 0.66666806, 0.        , 0.        ],\n        [0.        , 0.33333194, 0.66666806, 0.        , 0.        ]]),\n 'E:\\\\Databases\\\\NoXi\\\\NoXi_annotations_reliable_gold_standard_regression_with_additional_train_data\\\\English\\\\train\\\\052_2016-04-12_Nottingham\\\\annotation_novice.txt': array([[0.        , 0.        , 1.        , 0.        , 0.        ],\n        [0.        , 0.        , 1.        , 0.        , 0.        ],\n        [0.        , 0.09878004, 0.90121996, 0.        , 0.        ],\n        ...,\n        [0.        , 0.        , 1.        , 0.        , 0.        ],\n        [0.        , 0.        , 1.        , 0.        , 0.        ],\n        [0.        , 0.        , 1.        , 0.        , 0.        ]]),\n 'E:\\\\Databases\\\\NoXi\\\\NoXi_annotations_reliable_gold_standard_regression_with_additional_train_data\\\\French\\\\train\\\\001_2016-03-17_Paris\\\\annotation_expert.txt': array([[0.        , 0.        , 1.        , 0.        , 0.        ],\n        [0.        , 0.        , 1.        , 0.        , 0.        ],\n        [0.        , 0.        , 0.99405599, 0.00594401, 0.        ],\n        ...,\n        [0.        , 0.        , 1.        , 0.        , 0.        ],\n        [0.        , 0.        , 1.        , 0.        , 0.        ],\n        [0.        , 0.        , 1.        , 0.        , 0.        ]]),\n 'E:\\\\Databases\\\\NoXi\\\\NoXi_annotations_reliable_gold_standard_regression_with_additional_train_data\\\\French\\\\train\\\\001_2016-03-17_Paris\\\\annotation_novice.txt': array([[0.        , 0.25      , 0.75      , 0.        , 0.        ],\n        [0.        , 0.25      , 0.75      , 0.        , 0.        ],\n        [0.        , 0.23294795, 0.76705205, 0.        , 0.        ],\n        ...,\n        [0.        , 0.        , 1.        , 0.        , 0.        ],\n        [0.        , 0.        , 1.        , 0.        , 0.        ],\n        [0.        , 0.        , 1.        , 0.        , 0.        ]]),\n 'E:\\\\Databases\\\\NoXi\\\\NoXi_annotations_reliable_gold_standard_regression_with_additional_train_data\\\\French\\\\train\\\\004_2016-03-18_Paris\\\\annotation_expert.txt': array([[0.        , 0.        , 1.        , 0.        , 0.        ],\n        [0.        , 0.        , 1.        , 0.        , 0.        ],\n        [0.        , 0.89514005, 0.10485995, 0.        , 0.        ],\n        ...,\n        [0.        , 0.        , 1.        , 0.        , 0.        ],\n        [0.        , 0.        , 1.        , 0.        , 0.        ],\n        [0.        , 0.        , 1.        , 0.        , 0.        ]]),\n 'E:\\\\Databases\\\\NoXi\\\\NoXi_annotations_reliable_gold_standard_regression_with_additional_train_data\\\\French\\\\train\\\\004_2016-03-18_Paris\\\\annotation_novice.txt': array([[0.        , 0.        , 1.        , 0.        , 0.        ],\n        [0.        , 0.        , 1.        , 0.        , 0.        ],\n        [0.        , 0.08835995, 0.91164005, 0.        , 0.        ],\n        ...,\n        [0.        , 0.        , 1.        , 0.        , 0.        ],\n        [0.        , 0.        , 1.        , 0.        , 0.        ],\n        [0.        , 0.        , 1.        , 0.        , 0.        ]]),\n 'E:\\\\Databases\\\\NoXi\\\\NoXi_annotations_reliable_gold_standard_regression_with_additional_train_data\\\\French\\\\train\\\\005_2016-03-18_Paris\\\\annotation_expert.txt': array([[0., 0., 1., 0., 0.],\n        [0., 0., 1., 0., 0.],\n        [0., 0., 1., 0., 0.],\n        ...,\n        [0., 0., 1., 0., 0.],\n        [0., 0., 1., 0., 0.],\n        [0., 0., 1., 0., 0.]]),\n 'E:\\\\Databases\\\\NoXi\\\\NoXi_annotations_reliable_gold_standard_regression_with_additional_train_data\\\\French\\\\train\\\\005_2016-03-18_Paris\\\\annotation_novice.txt': array([[0., 0., 1., 0., 0.],\n        [0., 0., 1., 0., 0.],\n        [0., 0., 1., 0., 0.],\n        ...,\n        [0., 0., 1., 0., 0.],\n        [0., 0., 1., 0., 0.],\n        [0., 0., 1., 0., 0.]]),\n 'E:\\\\Databases\\\\NoXi\\\\NoXi_annotations_reliable_gold_standard_regression_with_additional_train_data\\\\French\\\\train\\\\006_2016-03-18_Paris\\\\annotation_expert.txt': array([[0., 0., 1., 0., 0.],\n        [0., 0., 1., 0., 0.],\n        [0., 0., 1., 0., 0.],\n        ...,\n        [0., 0., 1., 0., 0.],\n        [0., 0., 1., 0., 0.],\n        [0., 0., 1., 0., 0.]]),\n 'E:\\\\Databases\\\\NoXi\\\\NoXi_annotations_reliable_gold_standard_regression_with_additional_train_data\\\\French\\\\train\\\\006_2016-03-18_Paris\\\\annotation_novice.txt': array([[0., 0., 1., 0., 0.],\n        [0., 0., 1., 0., 0.],\n        [0., 0., 1., 0., 0.],\n        ...,\n        [0., 0., 1., 0., 0.],\n        [0., 0., 1., 0., 0.],\n        [0., 0., 1., 0., 0.]]),\n 'E:\\\\Databases\\\\NoXi\\\\NoXi_annotations_reliable_gold_standard_regression_with_additional_train_data\\\\French\\\\train\\\\009_2016-03-25_Paris\\\\annotation_expert.txt': array([[0., 0., 1., 0., 0.],\n        [0., 0., 1., 0., 0.],\n        [0., 0., 1., 0., 0.],\n        ...,\n        [0., 0., 1., 0., 0.],\n        [0., 0., 1., 0., 0.],\n        [0., 0., 1., 0., 0.]]),\n 'E:\\\\Databases\\\\NoXi\\\\NoXi_annotations_reliable_gold_standard_regression_with_additional_train_data\\\\French\\\\train\\\\009_2016-03-25_Paris\\\\annotation_novice.txt': array([[0., 0., 1., 0., 0.],\n        [0., 0., 1., 0., 0.],\n        [0., 0., 1., 0., 0.],\n        ...,\n        [0., 0., 1., 0., 0.],\n        [0., 0., 1., 0., 0.],\n        [0., 0., 1., 0., 0.]]),\n 'E:\\\\Databases\\\\NoXi\\\\NoXi_annotations_reliable_gold_standard_regression_with_additional_train_data\\\\French\\\\train\\\\010_2016-03-25_Paris\\\\annotation_expert.txt': array([[0. , 0. , 1. , 0. , 0. ],\n        [0. , 0. , 1. , 0. , 0. ],\n        [0. , 0. , 1. , 0. , 0. ],\n        ...,\n        [0. , 0.5, 0.5, 0. , 0. ],\n        [0. , 0.5, 0.5, 0. , 0. ],\n        [0. , 0.5, 0.5, 0. , 0. ]]),\n 'E:\\\\Databases\\\\NoXi\\\\NoXi_annotations_reliable_gold_standard_regression_with_additional_train_data\\\\French\\\\train\\\\010_2016-03-25_Paris\\\\annotation_novice.txt': array([[0. , 0. , 1. , 0. , 0. ],\n        [0. , 0. , 1. , 0. , 0. ],\n        [0. , 0. , 1. , 0. , 0. ],\n        ...,\n        [0. , 0.5, 0.5, 0. , 0. ],\n        [0. , 0.5, 0.5, 0. , 0. ],\n        [0. , 0. , 1. , 0. , 0. ]]),\n 'E:\\\\Databases\\\\NoXi\\\\NoXi_annotations_reliable_gold_standard_regression_with_additional_train_data\\\\French\\\\train\\\\011_2016-03-25_Paris\\\\annotation_expert.txt': array([[0.        , 0.        , 1.        , 0.        , 0.        ],\n        [0.        , 0.        , 1.        , 0.        , 0.        ],\n        [0.        , 0.        , 1.        , 0.        , 0.        ],\n        ...,\n        [0.        , 0.66666806, 0.33333194, 0.        , 0.        ],\n        [0.        , 0.66666806, 0.33333194, 0.        , 0.        ],\n        [0.        , 0.66666806, 0.33333194, 0.        , 0.        ]]),\n 'E:\\\\Databases\\\\NoXi\\\\NoXi_annotations_reliable_gold_standard_regression_with_additional_train_data\\\\French\\\\train\\\\011_2016-03-25_Paris\\\\annotation_novice.txt': array([[0.        , 0.        , 1.        , 0.        , 0.        ],\n        [0.        , 0.        , 1.        , 0.        , 0.        ],\n        [0.        , 0.66666806, 0.33333194, 0.        , 0.        ],\n        ...,\n        [0.        , 0.66666806, 0.33333194, 0.        , 0.        ],\n        [0.        , 0.66666806, 0.33333194, 0.        , 0.        ],\n        [1.        , 0.        , 0.        , 0.        , 0.        ]]),\n 'E:\\\\Databases\\\\NoXi\\\\NoXi_annotations_reliable_gold_standard_regression_with_additional_train_data\\\\French\\\\train\\\\012_2016-03-25_Paris\\\\annotation_expert.txt': array([[0., 0., 1., 0., 0.],\n        [0., 0., 1., 0., 0.],\n        [0., 0., 1., 0., 0.],\n        ...,\n        [0., 1., 0., 0., 0.],\n        [0., 1., 0., 0., 0.],\n        [0., 1., 0., 0., 0.]]),\n 'E:\\\\Databases\\\\NoXi\\\\NoXi_annotations_reliable_gold_standard_regression_with_additional_train_data\\\\French\\\\train\\\\012_2016-03-25_Paris\\\\annotation_novice.txt': array([[0., 0., 1., 0., 0.],\n        [0., 0., 1., 0., 0.],\n        [0., 0., 1., 0., 0.],\n        ...,\n        [0., 1., 0., 0., 0.],\n        [0., 1., 0., 0., 0.],\n        [0., 1., 0., 0., 0.]]),\n 'E:\\\\Databases\\\\NoXi\\\\NoXi_annotations_reliable_gold_standard_regression_with_additional_train_data\\\\French\\\\train\\\\013_2016-03-30_Paris\\\\annotation_expert.txt': array([[0., 0., 1., 0., 0.],\n        [0., 0., 1., 0., 0.],\n        [0., 0., 1., 0., 0.],\n        ...,\n        [0., 1., 0., 0., 0.],\n        [0., 1., 0., 0., 0.],\n        [0., 1., 0., 0., 0.]]),\n 'E:\\\\Databases\\\\NoXi\\\\NoXi_annotations_reliable_gold_standard_regression_with_additional_train_data\\\\French\\\\train\\\\013_2016-03-30_Paris\\\\annotation_novice.txt': array([[0., 0., 1., 0., 0.],\n        [0., 0., 1., 0., 0.],\n        [0., 0., 1., 0., 0.],\n        ...,\n        [1., 0., 0., 0., 0.],\n        [1., 0., 0., 0., 0.],\n        [1., 0., 0., 0., 0.]]),\n 'E:\\\\Databases\\\\NoXi\\\\NoXi_annotations_reliable_gold_standard_regression_with_additional_train_data\\\\French\\\\train\\\\014_2016-04-01_Paris\\\\annotation_expert.txt': array([[0. , 0. , 1. , 0. , 0. ],\n        [0. , 0. , 1. , 0. , 0. ],\n        [0. , 0. , 1. , 0. , 0. ],\n        ...,\n        [0. , 0. , 0.5, 0.5, 0. ],\n        [0. , 0. , 0.5, 0.5, 0. ],\n        [0. , 0. , 0.5, 0.5, 0. ]]),\n 'E:\\\\Databases\\\\NoXi\\\\NoXi_annotations_reliable_gold_standard_regression_with_additional_train_data\\\\French\\\\train\\\\014_2016-04-01_Paris\\\\annotation_novice.txt': array([[0. , 0. , 1. , 0. , 0. ],\n        [0. , 0. , 1. , 0. , 0. ],\n        [0. , 0. , 1. , 0. , 0. ],\n        ...,\n        [0. , 0. , 0.5, 0.5, 0. ],\n        [0. , 0. , 0.5, 0.5, 0. ],\n        [0. , 0. , 0.5, 0.5, 0. ]]),\n 'E:\\\\Databases\\\\NoXi\\\\NoXi_annotations_reliable_gold_standard_regression_with_additional_train_data\\\\German\\\\train\\\\067_2016-05-23_Augsburg\\\\annotation_novice.txt': array([[0. , 0. , 1. , 0. , 0. ],\n        [0. , 0. , 1. , 0. , 0. ],\n        [0. , 0. , 1. , 0. , 0. ],\n        ...,\n        [0. , 0.5, 0.5, 0. , 0. ],\n        [0. , 0.5, 0.5, 0. , 0. ],\n        [0. , 0.5, 0.5, 0. , 0. ]]),\n 'E:\\\\Databases\\\\NoXi\\\\NoXi_annotations_reliable_gold_standard_regression_with_additional_train_data\\\\German\\\\train\\\\070_2016-05-23_Augsburg\\\\annotation_expert.txt': array([[0., 0., 1., 0., 0.],\n        [0., 0., 1., 0., 0.],\n        [0., 0., 1., 0., 0.],\n        ...,\n        [1., 0., 0., 0., 0.],\n        [1., 0., 0., 0., 0.],\n        [1., 0., 0., 0., 0.]]),\n 'E:\\\\Databases\\\\NoXi\\\\NoXi_annotations_reliable_gold_standard_regression_with_additional_train_data\\\\German\\\\train\\\\070_2016-05-23_Augsburg\\\\annotation_novice.txt': array([[0., 0., 1., 0., 0.],\n        [0., 0., 1., 0., 0.],\n        [0., 0., 1., 0., 0.],\n        ...,\n        [1., 0., 0., 0., 0.],\n        [1., 0., 0., 0., 0.],\n        [0., 1., 0., 0., 0.]]),\n 'E:\\\\Databases\\\\NoXi\\\\NoXi_annotations_reliable_gold_standard_regression_with_additional_train_data\\\\German\\\\train\\\\073_2016-05-23_Augsburg\\\\annotation_expert.txt': array([[0. , 0. , 1. , 0. , 0. ],\n        [0. , 0. , 1. , 0. , 0. ],\n        [0. , 0. , 1. , 0. , 0. ],\n        ...,\n        [0. , 0. , 0.5, 0.5, 0. ],\n        [0. , 0. , 0.5, 0.5, 0. ],\n        [0. , 0. , 0.5, 0.5, 0. ]]),\n 'E:\\\\Databases\\\\NoXi\\\\NoXi_annotations_reliable_gold_standard_regression_with_additional_train_data\\\\German\\\\train\\\\076_2016-05-24_Augsburg\\\\annotation_expert.txt': array([[0. , 0. , 1. , 0. , 0. ],\n        [0. , 0. , 1. , 0. , 0. ],\n        [0. , 0. , 1. , 0. , 0. ],\n        ...,\n        [0. , 0.5, 0.5, 0. , 0. ],\n        [0. , 0.5, 0.5, 0. , 0. ],\n        [0. , 0.5, 0.5, 0. , 0. ]]),\n 'E:\\\\Databases\\\\NoXi\\\\NoXi_annotations_reliable_gold_standard_regression_with_additional_train_data\\\\German\\\\train\\\\076_2016-05-24_Augsburg\\\\annotation_novice.txt': array([[0., 0., 1., 0., 0.],\n        [0., 0., 1., 0., 0.],\n        [0., 0., 1., 0., 0.],\n        ...,\n        [0., 1., 0., 0., 0.],\n        [0., 1., 0., 0., 0.],\n        [0., 1., 0., 0., 0.]]),\n 'E:\\\\Databases\\\\NoXi\\\\NoXi_annotations_reliable_gold_standard_regression_with_additional_train_data\\\\German\\\\train\\\\080_2016-05-24_Augsburg\\\\annotation_expert.txt': array([[0.        , 0.        , 1.        , 0.        , 0.        ],\n        [0.        , 0.        , 1.        , 0.        , 0.        ],\n        [0.        , 0.        , 1.        , 0.        , 0.        ],\n        ...,\n        [0.        , 0.33333194, 0.66666806, 0.        , 0.        ],\n        [0.        , 0.33333194, 0.66666806, 0.        , 0.        ],\n        [0.        , 0.33333194, 0.66666806, 0.        , 0.        ]]),\n 'E:\\\\Databases\\\\NoXi\\\\NoXi_annotations_reliable_gold_standard_regression_with_additional_train_data\\\\German\\\\train\\\\080_2016-05-24_Augsburg\\\\annotation_novice.txt': array([[0.        , 0.        , 1.        , 0.        , 0.        ],\n        [0.        , 0.        , 1.        , 0.        , 0.        ],\n        [0.        , 0.        , 1.        , 0.        , 0.        ],\n        ...,\n        [0.        , 0.33333194, 0.66666806, 0.        , 0.        ],\n        [0.        , 0.33333194, 0.66666806, 0.        , 0.        ],\n        [0.        , 0.33333194, 0.66666806, 0.        , 0.        ]]),\n 'E:\\\\Databases\\\\NoXi\\\\NoXi_annotations_reliable_gold_standard_regression_with_additional_train_data\\\\German\\\\train\\\\081_2016-05-24_Augsburg\\\\annotation_expert.txt': array([[0.        , 0.        , 1.        , 0.        , 0.        ],\n        [0.        , 0.        , 1.        , 0.        , 0.        ],\n        [0.        , 0.        , 1.        , 0.        , 0.        ],\n        ...,\n        [0.        , 0.        , 0.33333206, 0.66666794, 0.        ],\n        [0.        , 0.        , 0.33333206, 0.66666794, 0.        ],\n        [0.        , 0.        , 0.33333206, 0.66666794, 0.        ]]),\n 'E:\\\\Databases\\\\NoXi\\\\NoXi_annotations_reliable_gold_standard_regression_with_additional_train_data\\\\German\\\\train\\\\081_2016-05-24_Augsburg\\\\annotation_novice.txt': array([[0.        , 0.        , 1.        , 0.        , 0.        ],\n        [0.        , 0.        , 1.        , 0.        , 0.        ],\n        [0.        , 0.        , 1.        , 0.        , 0.        ],\n        ...,\n        [0.        , 0.33333194, 0.66666806, 0.        , 0.        ],\n        [0.        , 0.33333194, 0.66666806, 0.        , 0.        ],\n        [0.        , 0.33333194, 0.66666806, 0.        , 0.        ]]),\n 'E:\\\\Databases\\\\NoXi\\\\NoXi_annotations_reliable_gold_standard_regression_with_additional_train_data\\\\German\\\\train\\\\082_2016-05-25_Augsburg\\\\annotation_expert.txt': array([[0., 0., 1., 0., 0.],\n        [0., 0., 1., 0., 0.],\n        [0., 0., 1., 0., 0.],\n        ...,\n        [1., 0., 0., 0., 0.],\n        [1., 0., 0., 0., 0.],\n        [1., 0., 0., 0., 0.]]),\n 'E:\\\\Databases\\\\NoXi\\\\NoXi_annotations_reliable_gold_standard_regression_with_additional_train_data\\\\German\\\\train\\\\082_2016-05-25_Augsburg\\\\annotation_novice.txt': array([[0., 0., 1., 0., 0.],\n        [0., 0., 1., 0., 0.],\n        [0., 0., 1., 0., 0.],\n        ...,\n        [1., 0., 0., 0., 0.],\n        [1., 0., 0., 0., 0.],\n        [1., 0., 0., 0., 0.]]),\n 'E:\\\\Databases\\\\NoXi\\\\NoXi_annotations_reliable_gold_standard_regression_with_additional_train_data\\\\German\\\\train\\\\083_2016-05-25_Augsburg\\\\annotation_expert.txt': array([[0., 0., 1., 0., 0.],\n        [0., 0., 1., 0., 0.],\n        [0., 0., 1., 0., 0.],\n        ...,\n        [0., 0., 1., 0., 0.],\n        [0., 0., 1., 0., 0.],\n        [0., 0., 1., 0., 0.]]),\n 'E:\\\\Databases\\\\NoXi\\\\NoXi_annotations_reliable_gold_standard_regression_with_additional_train_data\\\\German\\\\train\\\\083_2016-05-25_Augsburg\\\\annotation_novice.txt': array([[0., 0., 1., 0., 0.],\n        [0., 0., 1., 0., 0.],\n        [0., 0., 1., 0., 0.],\n        ...,\n        [0., 0., 0., 1., 0.],\n        [0., 0., 0., 1., 0.],\n        [0., 0., 0., 1., 0.]])}"
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_train_labels_converted"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h2> Labels softening (dev and test) </h2>"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "outputs": [],
   "source": [
    "# PROVED\n",
    "# function for calculation the one-got encoding vector according to its neighborhood\n",
    "def get_one_hot_encoding_according_to_neighborhood(array:np.ndarray, point_idx:int)->np.ndarray:\n",
    "    \"\"\" calculate hard one-hot encoding based on the neighborhood of the point.\n",
    "    The value will be calculated as a mode of the whole window, which is presented by array.\n",
    "    The array should be passed as a 2d-ndarray, representing the one-hot encoding vectors\n",
    "\n",
    "    :param array: np.ndarray\n",
    "            2d np.ndarray, representing the \"array\" of the one-hot encoding vectors\n",
    "    :param point_idx: int\n",
    "            index of the point, for which the one-hot encoding should be calculated.\n",
    "    :return: np.ndarray\n",
    "            1d np.ndarray. Calculated as a mode of the whole array (window) and represented as one-hot encoding vector\n",
    "    \"\"\"\n",
    "    # save the length of the one-hot encoding vectors\n",
    "    length_one_hot=array.shape[1]\n",
    "    # delete the point from the consideration of evaluation\n",
    "    array=np.delete(array, point_idx, axis=0)\n",
    "    # delete all points, which can be edge points (then their hard encodings contain only zeros)\n",
    "    array=array[~np.all(array == 0, axis=1)]\n",
    "    # convert the array of one/hot encodings to the class numbers\n",
    "    array=np.argmax(array, axis=1)\n",
    "    # evaluate the mode of the array\n",
    "    mode=int(stats.mode(array, axis=None)[0])\n",
    "    # convert found class for the point to one-hot encoding\n",
    "    result=np.zeros((length_one_hot,))\n",
    "    result[mode]=1.\n",
    "    return result\n",
    "\n",
    "def convert_points_according_to_neighborhood(array:np.ndarray, point_idxes:np.ndarray, neighborhood_length:int)->np.ndarray:\n",
    "    \"\"\"Convert points in the array according to their neighborhood (as a mode of their neighborhood)\n",
    "\n",
    "    :param neighborhood_length: int\n",
    "            the half-length of the \"window\", in which the mode should be calculated.\n",
    "            Passed as a half of the window to take from the left and right side the same number of neighbors.\n",
    "    :param array: np.ndarray\n",
    "            np.ndarray, in which converting should be done.\n",
    "    :param point_idxes: np.ndarray\n",
    "            array of points' indexes, which should be changed.\n",
    "    :return: np.ndarray\n",
    "            np.ndarray with converted points\n",
    "    \"\"\"\n",
    "    # copy original array\n",
    "    array=array.copy()\n",
    "    # go through all points\n",
    "    for point_idx in point_idxes:\n",
    "        # define the start and end indexes\n",
    "        start=point_idx-neighborhood_length\n",
    "        end=point_idx+neighborhood_length\n",
    "        # check if we are not outside the array\n",
    "        if start<0:\n",
    "            start=0\n",
    "            point_idx_in_window=point_idx # :), yes, I have written it to understand the logic\n",
    "        elif end>=array.shape[0]:\n",
    "            end=array.shape[0]-1\n",
    "            point_idx_in_window=neighborhood_length # new array will have the position for the point on the distance of neighborhood_length from start\n",
    "        else:\n",
    "            point_idx_in_window=neighborhood_length\n",
    "        # take the window for mode calculation\n",
    "        values=array[start:end+1]\n",
    "        result_point=get_one_hot_encoding_according_to_neighborhood(values,point_idx_in_window)\n",
    "        # change this point in the array\n",
    "        array[point_idx]=result_point\n",
    "\n",
    "    return array\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [],
   "source": [
    "# TODO: complete function\n",
    "# converting dev and test labels to the hard one-hot encodings\n",
    "def convert_labels_to_hard_one_hot_encodings(labels:Dict[str, np.ndarray], class_values:Tuple[float,...])->Dict[str, np.ndarray]:\n",
    "    \"\"\"Converts all labels presented in the Dict[path->values] to the hard one-hot encodings using provided class_values.\n",
    "\n",
    "    For the validation and test sets we need to have hard one-hot encodings, since it is not possible to assign some distinct class, when we have values such as 0.375 (it is right in the middle between two classes: 0.25 and 0.5).\n",
    "    For all other cases, the closest class will be chosen (for example, for value 0.4, then 0.5 class will be chosen).\n",
    "    Regarding these \"edge cases\" like 0.375, I want to go through them with the Hamming window.\n",
    "\n",
    "    :param labels: Dict[str, np.ndarray]\n",
    "            labels in the format Dict[path->np.ndarray]\n",
    "    :param class_values: Tuple[float,...]\n",
    "            Possible values of the classes, ranked in ascended order\n",
    "    :return: Dict[str, np.ndarray]\n",
    "            converted to the soft one-hot encodings labels\n",
    "    \"\"\"\n",
    "    # make a copy to not change the original data\n",
    "    labels=labels.copy()\n",
    "    # calculate possible edge points\n",
    "    possible_edge_points=(np.array(class_values)[:-1]+np.array(class_values)[1:])/2.\n",
    "    # detect all \"edge points\"\n",
    "    edge_points_idx={}\n",
    "    for key, values in labels.items():\n",
    "        edge_points_idx[key]=np.isin(values, possible_edge_points)\n",
    "    # change all labels according to the closeness to the nearest class\n",
    "    for key, values in labels.items():\n",
    "        # convert to the soft one-hot encoding vector\n",
    "        result = np.array([convert_to_soft_one_hot_encoding(x, class_values) for x in values])\n",
    "        # make np.round to every value so that vectors become as hard one-hot encoding.\n",
    "        result = np.round(result)\n",
    "        # done\n",
    "        labels[key] = result\n",
    "    # change all \"edge points\" based on the mode of the window (10 past frames and 10 next frames)\n",
    "    for key, values in labels.items():\n",
    "        pass\n",
    "\n",
    "    pass"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "1\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "# tests for convert_points_according_to_neighborhood function\n",
    "array=np.array([[0., 0, 1., 0, 0],      # 2\n",
    "               [1., 0, 0., 0, 0],       # 0\n",
    "               [1., 0, 0, 0, 0],        # 0\n",
    "               [1, 0, 0., 0, 0],        # 0\n",
    "               [0, 1., 0, 0, 0],        # 1\n",
    "               [0, 0, 0.5, 0.5, 0],     # HZ -> 0\n",
    "               [0, 0., 0.5, 0.5, 0],    # HZ -> 0\n",
    "               [0, 0, 0.5, 0.5, 0],     # HZ -> 1\n",
    "               [0, 0, 1., 0, 0],        # 2\n",
    "               [0, 0, 0.5, 0.5, 0.],    # HZ -> 2\n",
    "               [0, 0, 0, 1., 0]]        # 3\n",
    "               )\n",
    "point_idxes=np.array([5])\n",
    "print(np.argmax(convert_points_according_to_neighborhood(np.round(array), point_idxes, neighborhood_length=3), axis=1)[5])\n",
    "\n",
    "point_idxes=np.array([6])\n",
    "print(np.argmax(convert_points_according_to_neighborhood(np.round(array), point_idxes, neighborhood_length=3), axis=1)[6])\n",
    "\n",
    "point_idxes=np.array([7])\n",
    "print(np.argmax(convert_points_according_to_neighborhood(np.round(array), point_idxes, neighborhood_length=3), axis=1)[7])\n",
    "\n",
    "point_idxes=np.array([9])\n",
    "print(np.argmax(convert_points_according_to_neighborhood(np.round(array), point_idxes, neighborhood_length=3), axis=1)[9])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "0\n",
      "0\n",
      "[0 3 0 3 0 3 4 3 3 0 0 0 2 4 1 0 0 0] \n",
      "\n",
      "[3 3 0 3 3 3 4 3 3 3 3 0 2 4 1 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "array=np.array([[0., 0.5, 0.5, 0, 0],       # HZ -> 3\n",
    "               [0., 0, 0., 1, 0],           # 3\n",
    "               [1., 0, 0, 0, 0],            # 0\n",
    "               [0., 0, 0., 1, 0],           # 3\n",
    "               [0.5, 0.5, 0, 0, 0],         # HZ -> 3\n",
    "               [0, 0, 0, 1., 0],            # 3\n",
    "               [0, 0., 0, 0, 1.],           # 4\n",
    "               [0, 0, 0, 1., 0],            # 3\n",
    "               [0, 0, 0., 1, 0],            # 3\n",
    "               [0, 0.5, 0.5, 0., 0.],       # HZ -> 3\n",
    "               [0, 0.5, 0.5, 0., 0],        # HZ -> 3\n",
    "               [1, 0, 0, 0., 0.],           # 0\n",
    "               [0, 0, 1, 0., 0.],           # 2\n",
    "               [0, 0, 0, 0., 1.],           # 4\n",
    "               [0, 1, 0, 0., 0.],           # 2\n",
    "               [0, 0, 0, 0.5, 0.],          # HZ -> 0\n",
    "               [1, 0, 0, 0., 0.],           # 0\n",
    "               [0, 0, 0, 0.5, 0.5]],        # HZ -> 0 (because previous one HZ turned out to be 0)\n",
    "               )\n",
    "point_idxes=np.array([0])\n",
    "print(np.argmax(convert_points_according_to_neighborhood(np.round(array), point_idxes, neighborhood_length=5), axis=1)[0])\n",
    "\n",
    "point_idxes=np.array([4])\n",
    "print(np.argmax(convert_points_according_to_neighborhood(np.round(array), point_idxes, neighborhood_length=5), axis=1)[4])\n",
    "\n",
    "point_idxes=np.array([9])\n",
    "print(np.argmax(convert_points_according_to_neighborhood(np.round(array), point_idxes, neighborhood_length=5), axis=1)[9])\n",
    "\n",
    "point_idxes=np.array([10])\n",
    "print(np.argmax(convert_points_according_to_neighborhood(np.round(array), point_idxes, neighborhood_length=5), axis=1)[10])\n",
    "\n",
    "point_idxes=np.array([15])\n",
    "print(np.argmax(convert_points_according_to_neighborhood(np.round(array), point_idxes, neighborhood_length=5), axis=1)[15])\n",
    "\n",
    "point_idxes=np.array([17])\n",
    "print(np.argmax(convert_points_according_to_neighborhood(np.round(array), point_idxes, neighborhood_length=5), axis=1)[17])\n",
    "\n",
    "print(np.argmax(np.round(array), axis=1),'\\n')\n",
    "print(np.argmax(convert_points_according_to_neighborhood(np.round(array), np.array([0,4,9,10,15,17]), neighborhood_length=5), axis=1))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}