{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from typing import Dict, Tuple, Callable, List\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from scipy.stats import stats\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as T\n",
    "from torchvision.io import read_image\n",
    "\n",
    "from pytorch_utils.data_preprocessing import convert_image_to_float_and_scale\n",
    "from pytorch_utils.models.input_preprocessing import resize_image_saving_aspect_ratio, EfficientNet_image_preprocessor\n",
    "from pytorch_utils.models.CNN_models import Modified_EfficientNet_B1, Modified_EfficientNet_B4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_preprocessing_functions(model_type):\n",
    "    if model_type not in ['EfficientNet-B1', 'EfficientNet-B4', 'Modified_HRNet']:\n",
    "        raise ValueError('The model type should be either \"EfficientNet-B1\", \"EfficientNet-B4\" or \"Modified_HRNet\".')\n",
    "    # define preprocessing functions\n",
    "    if model_type == 'EfficientNet-B1':\n",
    "        preprocessing_functions = [partial(resize_image_saving_aspect_ratio, expected_size=240),\n",
    "                                   EfficientNet_image_preprocessor()]\n",
    "    elif model_type == 'EfficientNet-B4':\n",
    "        preprocessing_functions = [partial(resize_image_saving_aspect_ratio, expected_size=380),\n",
    "                                   EfficientNet_image_preprocessor()]\n",
    "    elif model_type == 'Modified_HRNet':\n",
    "        preprocessing_functions = [partial(resize_image_saving_aspect_ratio, expected_size=256),\n",
    "                                   convert_image_to_float_and_scale,\n",
    "                                   T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "                                   ]  # From HRNet\n",
    "    else:\n",
    "        raise ValueError(f'The model type should be either \"EfficientNet-B1\", \"EfficientNet-B4\", or \"Modified_HRNet\".'\n",
    "                         f'Got {model_type} instead.')\n",
    "    return preprocessing_functions"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_class_from_range(value):\n",
    "    if value > 0 and value<=3:\n",
    "        return 0\n",
    "    elif value > 3 and value<=7:\n",
    "        return 1\n",
    "    elif value > 7 and value<=10:\n",
    "        return 2\n",
    "    else:\n",
    "        raise ValueError(f'The value should be from the range 1 to 10. Got:', value)\n",
    "\n",
    "# data loading function\n",
    "def load_data(path_to_data:str)->pd.DataFrame:\n",
    "    # load csv file\n",
    "    data = pd.read_csv(path_to_data)\n",
    "    data = data[['path_to_frame','timestamp','engagement_hhi']]\n",
    "    # transform labels. They are now in range 1 to 10, while we need classes (0, 1, 2). We transform them in a way that:\n",
    "    # 1-3 means disengagement (class 0)\n",
    "    # 4-7 means neutral (class 1)\n",
    "    # 8-10 means engagement (class 2)\n",
    "    data['engagement_hhi'] = data['engagement_hhi'].apply(lambda x: get_class_from_range(x))\n",
    "    # cleaning the column names\n",
    "    data = data.rename(columns={\"path_to_frame\": \"path\",\n",
    "                                \"engagement_hhi\":\"label\"})\n",
    "    # transform data to Dict[str, pd.DataFrame], where str is the video name and the pd.DataFrame is paths to images with labels\n",
    "    separated_videos = {}\n",
    "    video_names = data['path'].apply(lambda x:x.split(\"/\")[-2])\n",
    "    video_names = video_names.unique()\n",
    "    for video_name in video_names:\n",
    "        separated_videos[video_name] = data[data['path'].str.contains('/' + video_name + '/')]\n",
    "    return separated_videos"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def create_model(model_type:str):\n",
    "    if model_type == \"EfficientNet-B1\":\n",
    "        model = Modified_EfficientNet_B1(embeddings_layer_neurons=256, num_classes=3,\n",
    "                                     num_regression_neurons=None)\n",
    "    elif model_type == \"EfficientNet-B4\":\n",
    "        model = Modified_EfficientNet_B4(embeddings_layer_neurons=256, num_classes=3,\n",
    "                                         num_regression_neurons=None)\n",
    "    else:\n",
    "        raise ValueError(\"Unknown model type: %s\" % model_type)\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def load_and_preprocess_image(path:str, preprocessing_functions:List[Callable])->torch.Tensor:\n",
    "    image = read_image(path)\n",
    "    for function in preprocessing_functions:\n",
    "        image = function(image)\n",
    "    return image"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def _evaluate_model_one_video(video:pd.DataFrame, *, model, preprocessing_functions, batch_size)->Tuple[int, int]:\n",
    "    labels = []\n",
    "    predictions = []\n",
    "    images = []\n",
    "    for idx in range(video.shape[0]):\n",
    "        path_to_image = video.iloc[idx,0]\n",
    "        label = video.iloc[idx,-1]\n",
    "        image = load_and_preprocess_image(path_to_image, preprocessing_functions)\n",
    "        images.append(image)\n",
    "        labels.append(label)\n",
    "    # predicting batch_wise, because it is faster\n",
    "    with torch.no_grad():\n",
    "        for idx in range(0, len(images), batch_size):\n",
    "            pred = model(images[idx:idx+batch_size])\n",
    "            # reallocate to CPU\n",
    "            pred = pred.cpu().numpy().squeeze()\n",
    "            predictions.append(pred)\n",
    "    # take mode for labels\n",
    "    labels = stats.mode(labels)\n",
    "    # sum up all prediction and take a softmax (since we have class probabilities, it is even smarter to do so to get the most probable label for entire video)\n",
    "    predictions = torch.cat(predictions, dim=0)\n",
    "    predictions = torch.sum(predictions, dim = -1).argmax().cpu().numpy().squeeze()\n",
    "\n",
    "    # return two values\n",
    "    return labels, predictions\n",
    "\n",
    "def evaluate_model(videos:Dict[str,pd.DataFrame], model, preprocessing_functions, batch_size)->None:\n",
    "    evaluation_metrics_classification = {'accuracy_classification': accuracy_score,\n",
    "                                     'precision_classification': partial(precision_score, average='macro'),\n",
    "                                     'recall_classification': partial(recall_score, average='macro'),\n",
    "                                     'f1_classification': partial(f1_score, average='macro'),\n",
    "                                     }\n",
    "    labels = []\n",
    "    predictions = []\n",
    "    for video in videos:\n",
    "        label, prediction = _evaluate_model_one_video(video, model=model, preprocessing_functions=preprocessing_functions, batch_size=batch_size)\n",
    "        predictions.append(prediction)\n",
    "        labels.append(label)\n",
    "\n",
    "    predictions = np.concatenate(predictions, axis=0)\n",
    "    labels = np.concatenate(labels, axis=0)\n",
    "    # calculate evaluation metrics\n",
    "    evaluation_metrics = {\n",
    "        metric: evaluation_metrics_classification[metric](labels, predictions)\n",
    "        for metric in evaluation_metrics_classification\n",
    "    }\n",
    "    # print evaluation metrics\n",
    "    for metric_name, metric_value in evaluation_metrics.items():\n",
    "        print(\"%s: %.4f\" % (metric_name, metric_value))\n",
    "    return evaluation_metrics"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# main\n",
    "# params\n",
    "model_type = \"EfficientNet-B1\"\n",
    "model_weights = \"deep-capybara-42.pth\" # TODO: complete it\n",
    "path_to_data = \"/media/external_hdd_1/MHHRI/mhhri/prepared_data/HHI_Ego_Recordings/faces/MHHRI_facial_labels.csv\"\n",
    "batch_size = 16\n",
    "preprocessing_functions = get_preprocessing_functions(model_type)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# create and load model\n",
    "model = create_model(model_type)\n",
    "model.load_state_dict(torch.load(model_weights))\n",
    "model = model.to(device)\n",
    "# load data\n",
    "data = load_data(path_to_data)\n",
    "# evaluate model\n",
    "metrics = evaluate_model(videos=data, model=model, preprocessing_functions=preprocessing_functions, batch_size=batch_size)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}