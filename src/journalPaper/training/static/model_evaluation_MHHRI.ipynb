{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append('/work/home/dsu/EngagementRecognition/')\n",
    "sys.path.append('/work/home/dsu/datatools/')\n",
    "sys.path.append('/work/home/dsu/simple-HRNet-master/')\n",
    "\n",
    "from functools import partial\n",
    "from typing import Dict, Tuple, Callable, List\n",
    "\n",
    "import torch\n",
    "import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from scipy.stats import stats\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as T\n",
    "from torchvision.io import read_image\n",
    "\n",
    "from pytorch_utils.data_preprocessing import convert_image_to_float_and_scale\n",
    "from pytorch_utils.models.input_preprocessing import resize_image_saving_aspect_ratio, EfficientNet_image_preprocessor\n",
    "from pytorch_utils.models.CNN_models import Modified_EfficientNet_B1, Modified_EfficientNet_B4\n",
    "from pytorch_utils.models.Pose_estimation.HRNet import Modified_HRNet\n",
    "from visualization.ConfusionMatrixVisualization import plot_and_save_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_preprocessing_functions(model_type):\n",
    "    if model_type not in ['EfficientNet-B1', 'EfficientNet-B4', 'Modified_HRNet']:\n",
    "        raise ValueError('The model type should be either \"EfficientNet-B1\", \"EfficientNet-B4\" or \"Modified_HRNet\".')\n",
    "    # define preprocessing functions\n",
    "    if model_type == 'EfficientNet-B1':\n",
    "        preprocessing_functions = [partial(resize_image_saving_aspect_ratio, expected_size=240),\n",
    "                                   EfficientNet_image_preprocessor()]\n",
    "    elif model_type == 'EfficientNet-B4':\n",
    "        preprocessing_functions = [partial(resize_image_saving_aspect_ratio, expected_size=380),\n",
    "                                   EfficientNet_image_preprocessor()]\n",
    "    elif model_type == 'Modified_HRNet':\n",
    "        preprocessing_functions = [partial(resize_image_saving_aspect_ratio, expected_size=256),\n",
    "                                   convert_image_to_float_and_scale,\n",
    "                                   T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "                                   ]  # From HRNet\n",
    "    else:\n",
    "        raise ValueError(f'The model type should be either \"EfficientNet-B1\", \"EfficientNet-B4\", or \"Modified_HRNet\".'\n",
    "                         f'Got {model_type} instead.')\n",
    "    return preprocessing_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_class_from_range(value):\n",
    "    if value > 0 and value<=3:\n",
    "        return 0\n",
    "    elif value > 3 and value<=7:\n",
    "        return 1\n",
    "    elif value > 7 and value<=10:\n",
    "        return 2\n",
    "    else:\n",
    "        raise ValueError(f'The value should be from the range 1 to 10. Got:', value)\n",
    "\n",
    "# data loading function\n",
    "def load_data(path_to_data:str)->Dict[str, pd.DataFrame]:\n",
    "    # load csv file\n",
    "    data = pd.read_csv(path_to_data)\n",
    "    data = data[['path_to_frame','timestamp','engagement_hhi']]\n",
    "    # transform labels. They are now in range 1 to 10, while we need classes (0, 1, 2). We transform them in a way that:\n",
    "    # 1-3 means disengagement (class 0)\n",
    "    # 4-7 means neutral (class 1)\n",
    "    # 8-10 means engagement (class 2)\n",
    "    data['engagement_hhi'] = data['engagement_hhi'].apply(lambda x: get_class_from_range(x))\n",
    "    # cleaning the column names\n",
    "    data = data.rename(columns={\"path_to_frame\": \"path\",\n",
    "                                \"engagement_hhi\":\"label\"})\n",
    "    # transform data to Dict[str, pd.DataFrame], where str is the video name and the pd.DataFrame is paths to images with labels\n",
    "    separated_videos = {}\n",
    "    video_names = data['path'].apply(lambda x:x.split(\"/\")[-2])\n",
    "    video_names = video_names.unique()\n",
    "    for video_name in video_names:\n",
    "        separated_videos[video_name] = data[data['path'].str.contains('/' + video_name + '/')]\n",
    "    return separated_videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def create_model(model_type:str):\n",
    "    if model_type == \"EfficientNet-B1\":\n",
    "        model = Modified_EfficientNet_B1(embeddings_layer_neurons=256, num_classes=3,\n",
    "                                     num_regression_neurons=None)\n",
    "    elif model_type == \"EfficientNet-B4\":\n",
    "        model = Modified_EfficientNet_B4(embeddings_layer_neurons=256, num_classes=3,\n",
    "                                         num_regression_neurons=None)\n",
    "    elif model_type == \"Modified_HRNet\":\n",
    "        model = Modified_HRNet(pretrained=True,\n",
    "                               path_to_weights=HRNET_WEIGHTS,\n",
    "                               embeddings_layer_neurons=256, num_classes=3,\n",
    "                               num_regression_neurons=None,\n",
    "                               consider_only_upper_body=True)\n",
    "    else:\n",
    "        raise ValueError(\"Unknown model type: %s\" % model_type)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def load_and_preprocess_image(path:str, preprocessing_functions:List[Callable])->torch.Tensor:\n",
    "    image = read_image(path)\n",
    "    for function in preprocessing_functions:\n",
    "        image = function(image)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def _evaluate_model_one_video(video:pd.DataFrame, *, model, preprocessing_functions, batch_size,\n",
    "                             device:torch.device)->Tuple[int, int]:\n",
    "    labels = []\n",
    "    predictions = []\n",
    "    images = []\n",
    "    for idx in range(video.shape[0]):\n",
    "        path_to_image = video.iloc[idx,0]\n",
    "        label = video.iloc[idx,-1]\n",
    "        image = load_and_preprocess_image(path_to_image, preprocessing_functions)\n",
    "        images.append(image)\n",
    "        labels.append(label)\n",
    "    # convert images and labels into numpy array\n",
    "    images = [np.array(image)[np.newaxis,...] for image in images]\n",
    "    labels = [np.array(label)[np.newaxis,...] for label in labels]\n",
    "    images = np.concatenate(images, axis=0)\n",
    "    labels = np.concatenate(labels, axis=0)\n",
    "    # if there is only one image for the entire video, skip it\n",
    "    if images.shape[0]==1:\n",
    "        return np.NaN, np.NaN\n",
    "    # predicting batch_wise, because it is faster\n",
    "    with torch.no_grad():\n",
    "        for i in range(0,images.shape[0],batch_size):\n",
    "            start = i\n",
    "            end = i + batch_size\n",
    "            # check if we have only one image in the last batch. If so, the BatchNorm will arise error.\n",
    "            # therefore, we include that last image in the previous batch and then just do continue on the last iteration\n",
    "            if end == images.shape[0]-1:\n",
    "                end+=1\n",
    "            data = images[start:end]\n",
    "            if data.shape[0]==1:\n",
    "                continue\n",
    "            data = torch.from_numpy(data)\n",
    "            data = data.to(device)\n",
    "            pred = model(data)\n",
    "            # reallocate to CPU\n",
    "            pred = pred.cpu().detach()\n",
    "            predictions.append(pred)\n",
    "    # take mode for labels\n",
    "    labels = stats.mode(labels, axis=None)[0][0]\n",
    "    # sum up all prediction and take a softmax (since we have class probabilities, it is even smarter to do so to get the most probable label for entire video)\n",
    "    predictions = torch.cat(predictions, dim=0)\n",
    "    predictions = torch.sum(predictions, dim = 0).argmax().cpu().numpy().squeeze()\n",
    "\n",
    "    # return two values\n",
    "    return labels, predictions\n",
    "\n",
    "def evaluate_model(videos:Dict[str,pd.DataFrame], model, preprocessing_functions, batch_size,\n",
    "                  device:torch.device)->None:\n",
    "    evaluation_metrics_classification = {'accuracy_classification': accuracy_score,\n",
    "                                     'precision_classification': partial(precision_score, average='macro'),\n",
    "                                     'recall_classification': partial(recall_score, average='macro'),\n",
    "                                     'f1_classification': partial(f1_score, average='macro'),\n",
    "                                     }\n",
    "    labels = []\n",
    "    predictions = []\n",
    "    for video_name, video in tqdm.tqdm(videos.items()):\n",
    "        #print(\"processing video:\", video_name)\n",
    "        l, p = _evaluate_model_one_video(video, model=model, preprocessing_functions=preprocessing_functions, \n",
    "                                                      batch_size=batch_size, device=device)\n",
    "        predictions.append(p)\n",
    "        labels.append(l)\n",
    "    \n",
    "\n",
    "    #[print(item) for item in predictions]\n",
    "    predictions = np.array(predictions).squeeze()\n",
    "    labels = np.array(labels).squeeze()\n",
    "    return predictions, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                               | 0/290 [00:00<?, ?it/s]/tmp/ipykernel_2791/3287903286.py:39: DeprecationWarning: Please use `mode` from the `scipy.stats` namespace, the `scipy.stats.stats` namespace is deprecated.\n",
      "  labels = stats.mode(labels, axis=None)[0][0]\n",
      "/tmp/ipykernel_2791/3287903286.py:39: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  labels = stats.mode(labels, axis=None)[0][0]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 290/290 [02:32<00:00,  1.90it/s]\n"
     ]
    }
   ],
   "source": [
    "# main for facial model evaluation\n",
    "# params\n",
    "model_type = \"EfficientNet-B1\"\n",
    "model_weights = \"/work/home/dsu/tmp/deep-capybara-42.pth\" # TODO: complete it\n",
    "path_to_data = \"/media/external_hdd_1/MHHRI/mhhri/prepared_data/HHI_Ego_Recordings/faces/MHHRI_facial_labels.csv\"\n",
    "batch_size = 64\n",
    "preprocessing_functions = get_preprocessing_functions(model_type)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# create and load model\n",
    "model = create_model(model_type)\n",
    "model.load_state_dict(torch.load(model_weights))\n",
    "model = model.to(device)\n",
    "# load data\n",
    "data = load_data(path_to_data) # Dict[str,pd.DataFrame]\n",
    "#data = {k: data[k] for k in list(data)[:10]}\n",
    "# evaluate model\n",
    "predictions, labels = evaluate_model(videos=data, model=model, preprocessing_functions=preprocessing_functions, batch_size=batch_size,\n",
    "                        device=device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_data = \"/media/external_hdd_1/MHHRI/mhhri/prepared_data/HHI_Ego_Recordings/faces/MHHRI_facial_labels.csv\"\n",
    "data = pd.read_csv(path_to_data)\n",
    "data = data[['path_to_frame','timestamp','engagement_hhi']]\n",
    "# transform labels. They are now in range 1 to 10, while we need classes (0, 1, 2). We transform them in a way that:\n",
    "# 1-3 means disengagement (class 0)\n",
    "# 4-7 means neutral (class 1)\n",
    "# 8-10 means engagement (class 2)\n",
    "#data['engagement_hhi'] = data['engagement_hhi'].apply(lambda x: get_class_from_range(x))\n",
    "# cleaning the column names\n",
    "data = data.rename(columns={\"path_to_frame\": \"path\",\n",
    "                            \"engagement_hhi\":\"label\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/media/external_hdd_2/MHHRI/mhhri/prepared_dat...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/media/external_hdd_2/MHHRI/mhhri/prepared_dat...</td>\n",
       "      <td>0.20</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/media/external_hdd_2/MHHRI/mhhri/prepared_dat...</td>\n",
       "      <td>0.40</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/media/external_hdd_2/MHHRI/mhhri/prepared_dat...</td>\n",
       "      <td>0.60</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/media/external_hdd_2/MHHRI/mhhri/prepared_dat...</td>\n",
       "      <td>0.80</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77691</th>\n",
       "      <td>/media/external_hdd_2/MHHRI/mhhri/prepared_dat...</td>\n",
       "      <td>31.23</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77692</th>\n",
       "      <td>/media/external_hdd_2/MHHRI/mhhri/prepared_dat...</td>\n",
       "      <td>31.43</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77693</th>\n",
       "      <td>/media/external_hdd_2/MHHRI/mhhri/prepared_dat...</td>\n",
       "      <td>31.63</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77694</th>\n",
       "      <td>/media/external_hdd_2/MHHRI/mhhri/prepared_dat...</td>\n",
       "      <td>31.83</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77695</th>\n",
       "      <td>/media/external_hdd_2/MHHRI/mhhri/prepared_dat...</td>\n",
       "      <td>32.03</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>77696 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    path  timestamp  label\n",
       "0      /media/external_hdd_2/MHHRI/mhhri/prepared_dat...       0.00      5\n",
       "1      /media/external_hdd_2/MHHRI/mhhri/prepared_dat...       0.20      5\n",
       "2      /media/external_hdd_2/MHHRI/mhhri/prepared_dat...       0.40      5\n",
       "3      /media/external_hdd_2/MHHRI/mhhri/prepared_dat...       0.60      5\n",
       "4      /media/external_hdd_2/MHHRI/mhhri/prepared_dat...       0.80      5\n",
       "...                                                  ...        ...    ...\n",
       "77691  /media/external_hdd_2/MHHRI/mhhri/prepared_dat...      31.23      8\n",
       "77692  /media/external_hdd_2/MHHRI/mhhri/prepared_dat...      31.43      8\n",
       "77693  /media/external_hdd_2/MHHRI/mhhri/prepared_dat...      31.63      8\n",
       "77694  /media/external_hdd_2/MHHRI/mhhri/prepared_dat...      31.83      8\n",
       "77695  /media/external_hdd_2/MHHRI/mhhri/prepared_dat...      32.03      8\n",
       "\n",
       "[77696 rows x 3 columns]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_classification: 0.3737\n",
      "precision_classification: 0.1869\n",
      "recall_classification: 0.5000\n",
      "f1_classification: 0.2720\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dsu/anaconda3/envs/emotion_recognition_env/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Shape of passed values is (2, 2), indices imply (3, 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2791/1201589180.py\u001b[0m in \u001b[0;36m<cell line: 18>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s: %.4f\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmetric_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# plot confusion matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m plot_and_save_confusion_matrix(y_true=labels, y_pred=predictions, name_labels=['disengaged', 'neutral', 'engaged'],\n\u001b[0m\u001b[1;32m     19\u001b[0m                                path_to_save='/work/home/dsu/tmp/', name_filename='MHHRI_f2f_cm.png')\n\u001b[1;32m     20\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mevaluation_metrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/work/home/dsu/datatools/visualization/ConfusionMatrixVisualization.py\u001b[0m in \u001b[0;36mplot_and_save_confusion_matrix\u001b[0;34m(y_true, y_pred, name_labels, path_to_save, name_filename, title)\u001b[0m\n\u001b[1;32m     46\u001b[0m     \"\"\"\n\u001b[1;32m     47\u001b[0m     \u001b[0mc_m\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m     \u001b[0mconf_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_m\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/emotion_recognition_env/lib/python3.10/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    692\u001b[0m                 )\n\u001b[1;32m    693\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 694\u001b[0;31m                 mgr = ndarray_to_mgr(\n\u001b[0m\u001b[1;32m    695\u001b[0m                     \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    696\u001b[0m                     \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/emotion_recognition_env/lib/python3.10/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36mndarray_to_mgr\u001b[0;34m(values, index, columns, dtype, copy, typ)\u001b[0m\n\u001b[1;32m    349\u001b[0m     )\n\u001b[1;32m    350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 351\u001b[0;31m     \u001b[0m_check_values_indices_shape_match\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    352\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtyp\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"array\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/emotion_recognition_env/lib/python3.10/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36m_check_values_indices_shape_match\u001b[0;34m(values, index, columns)\u001b[0m\n\u001b[1;32m    420\u001b[0m         \u001b[0mpassed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m         \u001b[0mimplied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 422\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Shape of passed values is {passed}, indices imply {implied}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    423\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Shape of passed values is (2, 2), indices imply (3, 3)"
     ]
    }
   ],
   "source": [
    "evaluation_metrics_classification = {'accuracy_classification': accuracy_score,\n",
    "                                     'precision_classification': partial(precision_score, average='macro'),\n",
    "                                     'recall_classification': partial(recall_score, average='macro'),\n",
    "                                     'f1_classification': partial(f1_score, average='macro'),\n",
    "                                     }\n",
    "\n",
    "# filter out NaNs\n",
    "labels, predictions = labels[~np.isnan(labels)], predictions[~np.isnan(labels)]\n",
    "# calculate evaluation metrics\n",
    "evaluation_metrics = {\n",
    "    metric: evaluation_metrics_classification[metric](labels, predictions)\n",
    "    for metric in evaluation_metrics_classification\n",
    "}\n",
    "# print evaluation metrics\n",
    "for metric_name, metric_value in evaluation_metrics.items():\n",
    "    print(\"%s: %.4f\" % (metric_name, metric_value))\n",
    "# plot confusion matrix\n",
    "plot_and_save_confusion_matrix(y_true=labels, y_pred=predictions, name_labels=['disengaged', 'neutral', 'engaged'],\n",
    "                               path_to_save='/work/home/dsu/tmp/', name_filename='MHHRI_f2f_cm.png')\n",
    "return evaluation_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                               | 0/290 [00:00<?, ?it/s]/tmp/ipykernel_2791/614560276.py:39: DeprecationWarning: Please use `mode` from the `scipy.stats` namespace, the `scipy.stats.stats` namespace is deprecated.\n",
      "  labels = stats.mode(labels, axis=None)[0]\n",
      "/tmp/ipykernel_2791/614560276.py:39: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  labels = stats.mode(labels, axis=None)[0]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 290/290 [02:32<00:00,  1.90it/s]\n",
      "/tmp/ipykernel_2791/614560276.py:66: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  labels = np.array(labels).squeeze()\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "ufunc 'isnan' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2791/1413359650.py\u001b[0m in \u001b[0;36m<cell line: 17>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m#data = {k: data[k] for k in list(data)[:10]}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# evaluate model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m facial_metrics = evaluate_model(videos=data, model=model, preprocessing_functions=preprocessing_functions, batch_size=batch_size,\n\u001b[0m\u001b[1;32m     18\u001b[0m                         device=device)\n",
      "\u001b[0;32m/tmp/ipykernel_2791/614560276.py\u001b[0m in \u001b[0;36mevaluate_model\u001b[0;34m(videos, model, preprocessing_functions, batch_size, device)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;31m# filter out NaNs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m     \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m     \u001b[0;31m# calculate evaluation metrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     evaluation_metrics = {\n",
      "\u001b[0;31mTypeError\u001b[0m: ufunc 'isnan' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''"
     ]
    }
   ],
   "source": [
    "# main for facial model evaluation\n",
    "# params\n",
    "model_type = \"EfficientNet-B1\"\n",
    "model_weights = \"/work/home/dsu/tmp/deep-capybara-42.pth\" # TODO: complete it\n",
    "path_to_data = \"/media/external_hdd_1/MHHRI/mhhri/prepared_data/HHI_Ego_Recordings/faces/MHHRI_facial_labels.csv\"\n",
    "batch_size = 64\n",
    "preprocessing_functions = get_preprocessing_functions(model_type)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# create and load model\n",
    "model = create_model(model_type)\n",
    "model.load_state_dict(torch.load(model_weights))\n",
    "model = model.to(device)\n",
    "# load data\n",
    "data = load_data(path_to_data) # Dict[str,pd.DataFrame]\n",
    "#data = {k: data[k] for k in list(data)[:10]}\n",
    "# evaluate model\n",
    "facial_metrics = evaluate_model(videos=data, model=model, preprocessing_functions=preprocessing_functions, batch_size=batch_size,\n",
    "                        device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main for pose model evaluation\n",
    "# params\n",
    "HRNET_WEIGHTS = \"/work/home/dsu/simple-HRNet-master/pose_hrnet_w32_256x192.pth\"\n",
    "model_type = \"Modified_HRNet\"\n",
    "model_weights = \"/work/home/dsu/tmp/fresh-bush-43.pth\" # TODO: complete it\n",
    "path_to_data = \"/media/external_hdd_1/MHHRI/mhhri/prepared_data/HHI_Ego_Recordings/poses/MHHRI_pose_labels.csv\"\n",
    "batch_size = 64\n",
    "preprocessing_functions = get_preprocessing_functions(model_type)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# create and load model\n",
    "model = create_model(model_type)\n",
    "model.load_state_dict(torch.load(model_weights))\n",
    "model = model.to(device)\n",
    "# load data\n",
    "data = load_data(path_to_data) # Dict[str,pd.DataFrame]\n",
    "# evaluate model\n",
    "pose_metrics = evaluate_model(videos=data, model=model, preprocessing_functions=preprocessing_functions, batch_size=batch_size,\n",
    "                        device=device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
